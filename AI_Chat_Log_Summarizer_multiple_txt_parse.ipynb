{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6a0781",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb224b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7943d",
   "metadata": {},
   "source": [
    "# READ all text file from current folder and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1cf63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted messages:\n",
      "{'speaker': 'User', 'message': 'Hi, can you tell me about Python?'}\n",
      "{'speaker': 'AI', 'message': 'Sure! Python is a popular programming language known for its readability.'}\n",
      "{'speaker': 'User', 'message': 'What can I use it for?'}\n",
      "{'speaker': 'AI', 'message': 'You can use Python for web development, data analysis, AI, and more.'}\n",
      "{'speaker': 'User', 'message': 'Hello!'}\n",
      "{'speaker': 'AI', 'message': 'Hi! How can I assist you today?'}\n",
      "{'speaker': 'User', 'message': 'Can you explain what machine learning is?'}\n",
      "{'speaker': 'AI', 'message': 'Certainly! Machine learning is a field of AI that allows systems to learn from data.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder_path = 'chat_log'  # Change this to your folder\n",
    "text_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "all_messages = []\n",
    "for file_path in text_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        chat_txt = file.read()\n",
    "\n",
    "        pattern = r'(User|AI):\\s*(.*?)(?=\\n*User:|\\n*AI:|$)'\n",
    "        matches = re.findall(pattern, chat_txt, re.DOTALL)\n",
    "\n",
    "        for speaker, message in matches:\n",
    "            all_messages.append({\n",
    "                'speaker': speaker.strip(),\n",
    "                'message': message.strip()\n",
    "            })\n",
    "\n",
    "print(\"Extracted messages:\")\n",
    "for msg in all_messages:\n",
    "    print(msg)      # Print few messages for preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a5858",
   "metadata": {},
   "source": [
    "# Separate Speaker User and AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e9f3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated messages by speaker:\n",
      "User: 4 messages\n",
      "AI: 4 messages\n"
     ]
    }
   ],
   "source": [
    "speaker_messages = defaultdict(list)\n",
    "for msg in all_messages:\n",
    "    speaker_messages[msg['speaker']].append(msg['message'])\n",
    "\n",
    "print(\"Separated messages by speaker:\")\n",
    "for speaker, msgs in speaker_messages.items():\n",
    "    print(f\"{speaker}: {len(msgs)} messages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a457ba1",
   "metadata": {},
   "source": [
    "# Total Line of Message exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1d2ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total line of messege: 8\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Count messages and total exchanges (number of individual messages)\n",
    "total_messages = len(all_messages)\n",
    "print(f\"Total line of messege: {total_messages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b07f0",
   "metadata": {},
   "source": [
    "# combine USER + AI message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19397b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined user + AI text preview:\n",
      " Hi, can you tell me about Python? What can I use it for? Hello! Can you explain what machine learning is? Sure! Python is a popular programming language known for its readability. You can use Python for web development, data analysis, AI, and more. Hi! How can I assist you today? Certainly! Machine learning is a field of AI that allows systems to learn from data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "user_text = \" \".join(speaker_messages.get(\"User\", []))\n",
    "ai_text = \" \".join(speaker_messages.get(\"AI\", []))\n",
    "combined_text = user_text + \" \" + ai_text\n",
    "print(\"Combined user + AI text preview:\\n\", combined_text)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b9be4",
   "metadata": {},
   "source": [
    "# Remove punctuation and Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1340b138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text (after punctuation removal):\n",
      "['hi', 'can', 'you', 'tell', 'me', 'about', 'python', 'what', 'can', 'i', 'use', 'it', 'for', 'hello', 'can', 'you', 'explain', 'what', 'machine', 'learning', 'is', 'sure', 'python', 'is', 'a', 'popular', 'programming', 'language', 'known', 'for', 'its', 'readability', 'you', 'can', 'use', 'python', 'for', 'web', 'development', 'data', 'analysis', 'ai', 'and', 'more', 'hi', 'how', 'can', 'i', 'assist', 'you', 'today', 'certainly', 'machine', 'learning', 'is', 'a', 'field', 'of', 'ai', 'that', 'allows', 'systems', 'to', 'learn', 'from', 'data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "without_punc = str.maketrans('', '', string.punctuation)\n",
    "clean_text = combined_text.translate(without_punc)\n",
    "\n",
    "\n",
    "tokens = word_tokenize(clean_text.lower())\n",
    "\n",
    "print(\"Tokenized text (after punctuation removal):\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01adf9",
   "metadata": {},
   "source": [
    "# remove Stopword (used NLTK built in function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5376c4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stopword removal:\n",
      "['hi', 'tell', 'python', 'use', 'hello', 'explain', 'machine', 'learning', 'sure', 'python', 'popular', 'programming', 'language', 'known', 'readability', 'use', 'python', 'web', 'development', 'data', 'analysis', 'ai', 'hi', 'assist', 'today', 'certainly', 'machine', 'learning', 'field', 'ai', 'allows', 'systems', 'learn', 'data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "\n",
    "print(\"After stopword removal:\")\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d42e80",
   "metadata": {},
   "source": [
    "# lemmatization for better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830ce505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After POS-aware lemmatization:\n",
      "['hi', 'tell', 'python', 'use', 'hello', 'explain', 'machine', 'learning', 'sure', 'python', 'popular', 'programming', 'language', 'know', 'readability', 'use', 'python', 'web', 'development', 'data', 'analysis', 'ai', 'hi', 'assist', 'today', 'certainly', 'machine', 'learn', 'field', 'ai', 'allow', 'system', 'learn', 'data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def map_pos_to_wordnet(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "\n",
    "tagged = pos_tag(filtered_tokens)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [\n",
    "    lemmatizer.lemmatize(word, map_pos_to_wordnet(pos_tag))\n",
    "    for word, pos_tag in tagged\n",
    "]\n",
    "\n",
    "print(\"After POS-aware lemmatization:\")\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b10ec",
   "metadata": {},
   "source": [
    "# Applying TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f82ce249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL keywords (TF-IDF):\n",
      "['python', 'ai', 'data', 'hi', 'learn', 'machine', 'use', 'allow', 'analysis', 'assist', 'certainly', 'development', 'explain', 'field', 'hello', 'know', 'language', 'learning', 'popular', 'programming', 'readability', 'sure', 'system', 'tell', 'today', 'web']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_text = \" \".join(lemmatized_tokens)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([final_text])\n",
    "tfidf_scores = zip(vectorizer.get_feature_names_out(), X.toarray()[0])\n",
    "sorted_keywords = sorted(tfidf_scores, key=lambda x: -x[1])\n",
    "\n",
    "all_keywords = [word for word, score in sorted_keywords]\n",
    "\n",
    "print(\"ALL keywords (TF-IDF):\")\n",
    "print(all_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9858f3e5",
   "metadata": {},
   "source": [
    "# Final Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71491a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary:\n",
      "Total number of exchanges: 26\n",
      "The user asked mainly about python and ai\n",
      "Most common keywords: python, ai, data, hi, learn\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final Summary\n",
    "print(\"Final Summary:\")\n",
    "print(f\"Total number of exchanges: {len(all_keywords)}\")\n",
    "print(f\"The user asked mainly about {all_keywords[0]} and {all_keywords[1]}\")\n",
    "print(f\"Most common keywords: {', '.join(all_keywords[:5])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f6fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3facfad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
